{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ade218f-3c75-4a8d-ac61-20aaf603e547",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mindhive/nklab5/users/amarvi/anaconda3/envs/open-mmlab/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /mindhive/nklab5/users/amarvi/anaconda3/envs/open-mmlab/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIlEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/om2/user/amarvi/dino/')\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "\n",
    "import dobs.tools as tools\n",
    "import dobs.folder as folder\n",
    "import dobs.folder_list as folder_list\n",
    "# from dobs.utils import dobs_tranform\n",
    "\n",
    "import utils\n",
    "import vision_transformer as vits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b5ce9cf-8d36-4a72-9939-afb1e7511888",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "DOBS transform function (from dobs.utils)\n",
    "'''\n",
    "\n",
    "# image preprocessing steps     \n",
    "IMAGE_RESIZE=256\n",
    "IMAGE_SIZE=224\n",
    "GRAYSCALE_PROBABILITY=0.2\n",
    "resize_transform      = transforms.Resize(IMAGE_RESIZE)\n",
    "random_crop_transform = transforms.RandomCrop(IMAGE_SIZE)\n",
    "center_crop_transform = transforms.CenterCrop(IMAGE_SIZE)\n",
    "grayscale_transform   = transforms.RandomGrayscale(p=GRAYSCALE_PROBABILITY)\n",
    "normalize             = transforms.Normalize(mean=[0.5]*3,std=[0.5]*3)\n",
    "\n",
    "invert = transforms.RandomVerticalFlip(p=1.0)\n",
    "\n",
    "transform = transforms.Compose([resize_transform, \n",
    "                                            random_crop_transform, \n",
    "                                            grayscale_transform, \n",
    "                                            transforms.ToTensor(),\n",
    "                                            normalize,\n",
    "                                           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce04cfc0-436f-416d-8e68-9c4fad5b2f9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat = 'obj'\n",
    "all_acts = get_activations(dat=dat)\n",
    "perf_dict = run_svm(all_acts)\n",
    "\n",
    "with open('/om2/user/amarvi/FACE/saved_models/svm_perf/dino_%s.pkl'%dat, 'wb') as f:\n",
    "    pickle.dump(perf_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be2a84ba-e394-4411-98df-4fa1a1948234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(dat='face', arch='vit_small', patches=16):\n",
    "    arch = arch\n",
    "    patches = patches\n",
    "    dat = 'face'\n",
    "    ckpt_pth = f'/om2/user/amarvi/dino/saved_models/{dat}400_dino/checkpoint.pth'\n",
    "\n",
    "    test_data_dir=[f'/om2/group/nklab/shared/datasets/dobs_objface1000/{dat}s_1000/test/']\n",
    "    ImageFolder = folder_list.ImageFolder\n",
    "    dataset = ImageFolder(root=test_data_dir, \n",
    "                                  max_samples={f'{dat}s_1000': 10},\n",
    "                                  maxout=True,\n",
    "                                  read_seed=None,\n",
    "                                  transform=transform,\n",
    "                                  includePaths=False)\n",
    "    \n",
    "    data_loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                                      batch_size=10,\n",
    "                                                      shuffle=False,\n",
    "                                                      num_workers=4,\n",
    "                                                      pin_memory=True)\n",
    "    \n",
    "    \n",
    "    # load in model\n",
    "    model = vits.__dict__[arch](patch_size=patches, num_classes=0)\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    utils.load_pretrained_weights(model, ckpt_pth, 'student', arch, patches)\n",
    "\n",
    "    all_activations = np.empty((100, 10, 12, 384))\n",
    "    max_batches=100\n",
    "    for step, batch in enumerate(tqdm(data_loader, desc='act/grad')):\n",
    "        if max_batches is not None:\n",
    "            if step == max_batches:\n",
    "                break\n",
    "        x,y = batch\n",
    "        x = x.cuda()\n",
    "        with torch.no_grad():\n",
    "            out = model.get_intermediate_layers(x,n=12)\n",
    "            for idx, layer_activation in enumerate(out):\n",
    "                clss_token = layer_activation[:, 0, :].squeeze()\n",
    "                clss_token = clss_token.detach().cpu().numpy()\n",
    "                all_activations[step, :, idx, :] = clss_token\n",
    "\n",
    "    return all_activations.reshape((-1, 12, 384))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d67f901-873b-4dcd-b952-28b0ca7317bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_svm(acts):\n",
    "    \n",
    "    perf_dict = {}\n",
    "    \n",
    "    print('=========== starting SVM ===================')\n",
    "    for layer in range(acts.shape[1]):\n",
    "        act = acts[:, layer, :]\n",
    "        num_ids = 100\n",
    "        num_reps_id = 10\n",
    "        num_samples = num_ids*num_reps_id\n",
    "\n",
    "        indTest = np.arange(0,num_samples,num_reps_id)\n",
    "        indAll = np.arange(0,num_samples)\n",
    "        \n",
    "        x = np.arange(0,num_ids)\n",
    "        trainCat = np.repeat(x,num_reps_id-1)\n",
    "        perf_fold = np.zeros(shape=(num_reps_id,))\n",
    "        \n",
    "        for iFold in tqdm(range(num_reps_id)):\n",
    "            indTrain = np.setdiff1d(indAll,indTest+iFold)\n",
    "        \n",
    "            dataTest = act[indTest+iFold,:]\n",
    "            dataTrain = act[indTrain,:]\n",
    "            \n",
    "            clf = svm.LinearSVC(dual='auto')\n",
    "            clf.fit(dataTrain,trainCat)\n",
    "        \n",
    "            dec = clf.predict(dataTest)\n",
    "            \n",
    "            diff = dec - x\n",
    "            perf = np.where(diff == 0)[0]\n",
    "            perf = len(perf)/num_ids\n",
    "        \n",
    "            perf_fold[iFold] = perf\n",
    "            \n",
    "        perf_dict[layer] = perf_fold\n",
    "        print(layer, np.mean(perf_fold))\n",
    "\n",
    "    return perf_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
